{"cells":[{"cell_type":"markdown","source":["# Sequence to Sequence Learning with Neural Networks Implementation"],"metadata":{"id":"BfHg2m4EPrkY"}},{"cell_type":"markdown","source":["Dutch → English translation task"],"metadata":{"id":"2TzMd-ocRhs0"}},{"cell_type":"markdown","source":["## 1. Preprocessing"],"metadata":{"id":"nQ3L0hc0Pxnt"}},{"cell_type":"markdown","source":["spaCy: tokenization & tagging library"],"metadata":{"id":"GodMFFkAP9Kn"}},{"cell_type":"code","source":["!python -m spacy download en_core_web_sm # Engilish\n","!python -m spacy download nl_core_news_sm # Deustch"],"metadata":{"id":"gEDiUPanPE1v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spacy\n","\n","spacy_en = spacy.load(\"en_core_web_sm\")\n","spacy_de = spacy.load(\"nl_core_news_sm\")"],"metadata":{"id":"pxqzMlH_P7Vr","executionInfo":{"status":"ok","timestamp":1658685064743,"user_tz":-540,"elapsed":8936,"user":{"displayName":"김별희","userId":"06602448826503759202"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# tokenization test\n","tokenized = spacy_en.tokenizer(\"These days, i'm really into LostArk\")\n","\n","for i, token in enumerate(tokenized):\n","  print(f\"{i}번째 토큰: {token.text}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0SnDePvbQNs_","executionInfo":{"status":"ok","timestamp":1658685064743,"user_tz":-540,"elapsed":2,"user":{"displayName":"김별희","userId":"06602448826503759202"}},"outputId":"6c82b953-294a-4991-a47c-ab0bf0471ce4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["0번째 토큰: These\n","1번째 토큰: days\n","2번째 토큰: ,\n","3번째 토큰: i\n","4번째 토큰: 'm\n","5번째 토큰: really\n","6번째 토큰: into\n","7번째 토큰: LostArk\n"]}]},{"cell_type":"code","source":["# Tokenizer 함수 정의\n","\n","def tokenizer_de(text):\n","  \"\"\"\n","  논문에서 토큰의 순서를 거꾸로 뒤집어서 넣었을 때 성능이 향상됨을 보였으므로 토큰의 순서를 뒤집어서 반환\n","  \"\"\"\n","  return [token.text for token in spacy_de.tokenizer(text)][::-1]\n","\n","def tokenizer_en(text):\n","  return [token.text for token in spacy_en.tokenizer(text)]"],"metadata":{"id":"GyW2zoiRQ1F0","executionInfo":{"status":"ok","timestamp":1658685064744,"user_tz":-540,"elapsed":2,"user":{"displayName":"김별희","userId":"06602448826503759202"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["torchtext는 자연어 처리(NLP) 분야에서 사용하는 데이터로더(DataLoader)"],"metadata":{"id":"48k8X0jhVX-w"}},{"cell_type":"code","source":["pip install --user torchtext"],"metadata":{"id":"6GIzHtcCVa2H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torchtext\n","from torchtext.legacy.data import Field\n","# Field를 통해 앞으로 어떤 전처리를 할 것인지를 정의\n","\n","SRC = Field(tokenize=tokenizer_de, init_token=\"<SOS>\", eos_token=\"<EOS>\", lower=True)\n","TRG = Field(tokenize=tokenizer_en, init_token=\"<SOS>\", eos_token=\"<EOS>\", lower=True)"],"metadata":{"id":"fdUDUvAeRgEe","executionInfo":{"status":"ok","timestamp":1658685134382,"user_tz":-540,"elapsed":295,"user":{"displayName":"김별희","userId":"06602448826503759202"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## 2. Load Dataset"],"metadata":{"id":"cUzMHZMsUcp4"}},{"cell_type":"code","source":["from torchtext.legacy.datasets import Multi30k\n","\n","train_dataset, valid_dataset, test_dataset = Multi30k.splits(exts=(\".de\", \".en\"), fields=(SRC, TRG))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ysl1BzWSSLwX","executionInfo":{"status":"ok","timestamp":1658685205016,"user_tz":-540,"elapsed":10179,"user":{"displayName":"김별희","userId":"06602448826503759202"}},"outputId":"05d62d71-480a-40ba-d72d-383714d96b08"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["downloading training.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["training.tar.gz: 100%|██████████| 1.21M/1.21M [00:02<00:00, 564kB/s] \n"]},{"output_type":"stream","name":"stdout","text":["downloading validation.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 176kB/s]\n"]},{"output_type":"stream","name":"stdout","text":["downloading mmt_task1_test2016.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 165kB/s]\n"]}]},{"cell_type":"code","source":["print(f\"train dataset: {len(train_dataset.examples)}\")\n","print(f\"valid dataset: {len(valid_dataset.examples)}\")\n","print(f\"test dataset: {len(test_dataset.examples)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Uled879V33G","executionInfo":{"status":"ok","timestamp":1658685269680,"user_tz":-540,"elapsed":2,"user":{"displayName":"김별희","userId":"06602448826503759202"}},"outputId":"4c4a5d43-a4b2-488b-fee6-513029c29353"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["train dataset: 29000\n","valid dataset: 1014\n","test dataset: 1000\n"]}]},{"cell_type":"code","source":["# 독일어 문장과 영어 문장 샘플 출력\n","print(vars(train_dataset.examples[16])['src'])\n","print(vars(train_dataset.examples[16])['trg'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"08Za-HmHWKLU","executionInfo":{"status":"ok","timestamp":1658685325757,"user_tz":-540,"elapsed":2,"user":{"displayName":"김별희","userId":"06602448826503759202"}},"outputId":"64c8310b-e7c3-4e6f-ab00-c04d46e1dc54"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["['.', 'regenbogen', 'gemalten', 'großen', 'einem', 'vor', 'sitzt', 'mädchen', 'kleines', 'ein']\n","['a', 'little', 'girl', 'is', 'sitting', 'in', 'front', 'of', 'a', 'large', 'painted', 'rainbow', '.']\n"]}]},{"cell_type":"markdown","source":["최소 2번 이상 등장한 단어들을 이용해 영어와 독일어 단어 사전을 생성(field 객체의 build_vocab 메서드)"],"metadata":{"id":"zjvo7ZUwWgdW"}},{"cell_type":"code","source":["SRC.build_vocab(train_dataset, min_freq=2)\n","TRG.build_vocab(train_dataset, min_freq=2)\n","\n","print(f\"SRC vocab length: {len(SRC.vocab)}\")\n","print(f\"TRG vocab length: {len(TRG.vocab)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Xk3ZJUAWVgO","executionInfo":{"status":"ok","timestamp":1658685476835,"user_tz":-540,"elapsed":291,"user":{"displayName":"김별희","userId":"06602448826503759202"}},"outputId":"736ba743-6cea-42ff-d97f-d715054f1ccc"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["SRC vocab length: 7853\n","TRG vocab length: 5893\n"]}]},{"cell_type":"markdown","source":["논문에서 학습 속도를 위해 하나의 배치에 포함된 문장들이 가지는 단어의 개수가 유사하게 만들었다. <br/>\n","이를 구현하기 위해 BucketIterator를 사용한다."],"metadata":{"id":"gLDC15UbXMdj"}},{"cell_type":"code","source":["import torch\n","from torchtext.legacy.data import BucketIterator\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","BATCH_SIZE = 128\n","\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n","    (train_dataset, valid_dataset, test_dataset),\n","    batch_size=BATCH_SIZE,\n","    device = device\n",")"],"metadata":{"id":"nTR5rnBEW8vn","executionInfo":{"status":"ok","timestamp":1658685696134,"user_tz":-540,"elapsed":2,"user":{"displayName":"김별희","userId":"06602448826503759202"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["for i, batch in enumerate(train_iterator):\n","  src = batch.src\n","  trg = batch.trg\n","\n","  print(src.shape)\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BHgRjYpeXyW_","executionInfo":{"status":"ok","timestamp":1658685857220,"user_tz":-540,"elapsed":291,"user":{"displayName":"김별희","userId":"06602448826503759202"}},"outputId":"76e4c461-63e4-430c-91f4-a7ea4153d14f"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([30, 128])\n"]}]},{"cell_type":"markdown","source":["## 3. Model Architecture"],"metadata":{"id":"nsF5WgBpYb8L"}},{"cell_type":"markdown","source":["### Encoder"],"metadata":{"id":"AIjmipFRYfNF"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class Encoder(nn.Module):\n","\n","  def __init__(self, input_dim, embed_dim, hidden_dim, n_layers, dropout_ratio):\n","    super().__init__()\n","\n","    self.embedding = nn.Embedding(input_dim, embed_dim)\n","\n","    self.hidden_dim = hidden_dim\n","    self.n_layers = n_layers\n","    self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout_ratio)\n","\n","    self.dropout = nn.Dropout(dropout_ratio)\n","  \n","\n","  def forward(self, src):\n","    \"\"\"\n","      src 문장을 입력받아 context vector 반환\n","    \"\"\"\n","    # src = |단어 개수, bs|\n","    embedded = self.dropout(self.embedding(src)) # |단어 개수, bs, embed_dim|\n","\n","    outputs, (hidden, cell) = self.rnn(embedded)\n","    # outputs = |단어 개수, bs, hidden_dim|\n","    # hidden = |레이어 개수, bs, hidden_dim|\n","    # cell = [레이어 개수, bs, hidden_dim|\n","\n","    return hidden, cell"],"metadata":{"id":"6XPKrcD7YMZv","executionInfo":{"status":"ok","timestamp":1658688676426,"user_tz":-540,"elapsed":366,"user":{"displayName":"김별희","userId":"06602448826503759202"}}},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":["### Decoder"],"metadata":{"id":"L5DTsw-HardE"}},{"cell_type":"code","source":["class Decoder(nn.Module):\n","  \n","  def __init__(self, output_dim, embed_dim, hidden_dim, n_layers, dropout_ratio):\n","    super().__init__()\n","\n","    self.embedding = nn.Embedding(output_dim, embed_dim)\n","\n","    self.hidden_dim = hidden_dim\n","    self.n_layers = n_layers\n","    self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout_ratio)\n","\n","    # Encoder와의 차이점: FC layer\n","    self.output_dim = output_dim\n","    self.fc_out = nn.Linear(hidden_dim, output_dim)\n","\n","    self.dropout = nn.Dropout(dropout_ratio)\n","\n","\n","  def forward(self, input, hidden, cell):\n","    # input = |bs|\n","    # hidden = |레이어 개수, bs, hidden_dim|\n","    # cell = |레이어 개수, bs, hidden_dim|\n","    input = input.unsqueeze(0) # input = |1(단어 개수), bs|\n","\n","    embedded = self.dropout(self.embedding(input)) # embedded = |단어 개수, bs, embed_dim|\n","\n","    output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n","    # outputs = |1(단어 개수), bs, hidden_dim|\n","    # hidden = |레이어 개수, bs, hidden_dim|\n","    # cell = [레이어 개수, bs, hidden_dim|\n","\n","    pred = self.fc_out(output.squeeze(0)) # |bs, output_dim|\n","\n","    return pred, hidden, cell"],"metadata":{"id":"aaSWRx3kabAH","executionInfo":{"status":"ok","timestamp":1658688677457,"user_tz":-540,"elapsed":2,"user":{"displayName":"김별희","userId":"06602448826503759202"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":["### Seq2Seq"],"metadata":{"id":"wZ1udbpvckAJ"}},{"cell_type":"code","source":["import random"],"metadata":{"id":"6J7tRsQqeURM","executionInfo":{"status":"ok","timestamp":1658688678474,"user_tz":-540,"elapsed":1,"user":{"displayName":"김별희","userId":"06602448826503759202"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["class Seq2Seq(nn.Module):\n","\n","  def __init__(self, encoder, decoder, device):\n","    super().__init__()\n","\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    self.device = device\n","\n","  def forward(self, src, trg, teacher_forcing_ratio=0.5):\n","    # src = |단어 개수, bs|\n","    # trg = |단어 개수, bs|\n","\n","    hidden, cell = self.encoder(src)\n","\n","    trg_len = trg.shape[0] # 단어 개수\n","    batch_size = trg.shape[1] # bs\n","    trg_vocab_size = self.decoder.output_dim \n","    outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n","\n","    input = trg[0, :]\n","\n","    for t in range(1, trg_len):\n","      output, hidden, cell = self.decoder(input, hidden, cell)\n","\n","      outputs[t] = output # FC를 거쳐 나온 현재의 출력 단어 정보\n","      top1 = output.argmax(1) # 가장 확률이 높은 단어의 idx\n","\n","      # teacjer forcing 여부\n","      teacher_force = random.random() < teacher_forcing_ratio\n","      input = trg[t] if teacher_force else top1 \n","    \n","    return outputs"],"metadata":{"id":"neCH-rmRbLr3","executionInfo":{"status":"ok","timestamp":1658688679492,"user_tz":-540,"elapsed":2,"user":{"displayName":"김별희","userId":"06602448826503759202"}}},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":["## 4. Training"],"metadata":{"id":"fhKiZzAtegcf"}},{"cell_type":"code","source":["INPUT_DIM = len(SRC.vocab)\n","OUTPUT_DIM = len(TRG.vocab)\n","ENCODER_EMBED_DIM = 256\n","DEOCDER_EMBED_DIM = 256\n","HIDDEN_DIM = 512\n","N_LAYERS = 2\n","ENC_DROPOUT_RATIO = 0.5\n","DEC_DROPOUT_RATIO = 0.5"],"metadata":{"id":"1Yu6u1RDeceu","executionInfo":{"status":"ok","timestamp":1658688679492,"user_tz":-540,"elapsed":1,"user":{"displayName":"김별희","userId":"06602448826503759202"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["enc = Encoder(INPUT_DIM, ENCODER_EMBED_DIM, HIDDEN_DIM, N_LAYERS, ENC_DROPOUT_RATIO)\n","dec = Decoder(OUTPUT_DIM, DEOCDER_EMBED_DIM, HIDDEN_DIM, N_LAYERS, DEC_DROPOUT_RATIO)\n","\n","model = Seq2Seq(enc, dec, device).to(device)"],"metadata":{"id":"l5K5XRtuezwm","executionInfo":{"status":"ok","timestamp":1658688681043,"user_tz":-540,"elapsed":516,"user":{"displayName":"김별희","userId":"06602448826503759202"}}},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":["모델 가중치 파라미터 : (-0.08, 0.08)"],"metadata":{"id":"N7GHq-E3fVtO"}},{"cell_type":"code","source":["def init_weights(m):\n","  for name, param in m.named_parameters():\n","    nn.init.uniform(param.data, -0.08, 0.08)\n","\n","model.apply(init_weights)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JBBDK_jffET-","executionInfo":{"status":"ok","timestamp":1658688681043,"user_tz":-540,"elapsed":1,"user":{"displayName":"김별희","userId":"06602448826503759202"}},"outputId":"804f24d4-d096-416b-e481-d96791f2aadb"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"]},{"output_type":"execute_result","data":{"text/plain":["Seq2Seq(\n","  (encoder): Encoder(\n","    (embedding): Embedding(7853, 256)\n","    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n","  (decoder): Decoder(\n","    (embedding): Embedding(5893, 256)\n","    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n","    (fc_out): Linear(in_features=512, out_features=5893, bias=True)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n",")"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["import torch.optim as optim\n","\n","optimizer = optim.Adam(model.parameters())\n","\n","# padding 무시\n","TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n","criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"],"metadata":{"id":"ukSDZOnFfkYe","executionInfo":{"status":"ok","timestamp":1658688681874,"user_tz":-540,"elapsed":1,"user":{"displayName":"김별희","userId":"06602448826503759202"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["def train(model, iterator, optimizer, criterion, clip):\n","  model.train()\n","  epoch_loss = 0\n","\n","  for i, batch in enumerate(iterator):\n","    src = batch.src\n","    trg = batch.trg\n","\n","    optimizer.zero_grad()\n","\n","    output = model(src, trg) # |출력단어개수, bs, output_dim|\n","    output_dim = output.shape[-1]\n","\n","    output = output[1:].view(-1, output_dim) # |(출력 단어의 개수 - 1) * bs, output_dim|\n","    trg = trg[1:].view(-1) # |(타겟 단어의 개수 -1) * bs|\n","\n","    loss = criterion(output, trg)\n","    loss.backward()\n","\n","    torch.nn.utils.clip_grad_norm_(model.parameters(),clip)\n","\n","    optimizer.step()\n","\n","    epoch_loss += loss.item()\n","  \n","  return epoch_loss / len(iterator)"],"metadata":{"id":"a-2CM470f3l3","executionInfo":{"status":"ok","timestamp":1658688683622,"user_tz":-540,"elapsed":1,"user":{"displayName":"김별희","userId":"06602448826503759202"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, iterator, criterion):\n","  model.eval()\n","  epoch_loss = 0\n","\n","  with torch.no_grad():\n","    for i, batch in enumerate(iterator):\n","      src = batch.src\n","      trg = batch.trg\n","\n","      output = model(src, trg) # no teacher forcing\n","      output_dim = output.shape[-1]\n","\n","      output = output[1:].view(-1, output_dim)\n","      trg = trg[1:].view(-1)\n","\n","      loss = criterion(output, trg)\n","\n","      epoch_loss += loss.item()\n","  \n","  return epoch_loss / len(iterator)"],"metadata":{"id":"IdX58rOshApn","executionInfo":{"status":"ok","timestamp":1658688684115,"user_tz":-540,"elapsed":1,"user":{"displayName":"김별희","userId":"06602448826503759202"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["def epoch_time(start_time, end_time):\n","  elapsed_time = end_time - start_time\n","  elapsed_mins = int(elapsed_time / 60)\n","  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","  return elapsed_mins, elapsed_secs"],"metadata":{"id":"fHaWD8aMh2kn","executionInfo":{"status":"ok","timestamp":1658688684980,"user_tz":-540,"elapsed":2,"user":{"displayName":"김별희","userId":"06602448826503759202"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["import time\n","import math\n","\n","N_EPOCHS = 20\n","CLIP = 1\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","  start_time = time.time()\n","\n","  train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n","  valid_loss = evaluate(model, valid_iterator, criterion)\n","\n","  end_time = time.time()\n","  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","  if valid_loss < best_valid_loss:\n","    best_valid_loss = valid_loss\n","    torch.save(model.state_dict(), 'seq2seq_ver1.pt')\n","  \n","  print(f\"Epoch: {epoch+1} | Time: {epoch_mins}m {epoch_secs}s\")\n","  print(f\"Train loss: {train_loss} | Train perplexity: {math.exp(train_loss)}\")\n","  print(f\"Valid loss: {valid_loss} | Valid perplexity: {math.exp(valid_loss)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RBqcRSwdiKe_","executionInfo":{"status":"ok","timestamp":1658690280249,"user_tz":-540,"elapsed":569220,"user":{"displayName":"김별희","userId":"06602448826503759202"}},"outputId":"e53d910e-a562-408f-cb44-44a3bfbe25da"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 1 | Time: 0m 28s\n","Train loss: 2.9396154407887733 | Train perplexity: 18.908573447830648\n","Valid loss: 3.2289921045303345 | Valid perplexity: 25.254190555125582\n","Epoch: 2 | Time: 0m 29s\n","Train loss: 2.8466261515008195 | Train perplexity: 17.229553765336256\n","Valid loss: 3.007067322731018 | Valid perplexity: 20.2279906852768\n","Epoch: 3 | Time: 0m 28s\n","Train loss: 2.7633949996091194 | Train perplexity: 15.853574566696373\n","Valid loss: 2.8605929613113403 | Valid perplexity: 17.4718840168608\n","Epoch: 4 | Time: 0m 28s\n","Train loss: 2.693400189740017 | Train perplexity: 14.781851668951798\n","Valid loss: 3.063153862953186 | Valid perplexity: 21.394927537063577\n","Epoch: 5 | Time: 0m 28s\n","Train loss: 2.610236561771006 | Train perplexity: 13.602268247925894\n","Valid loss: 2.827407330274582 | Valid perplexity: 16.90158374524378\n","Epoch: 6 | Time: 0m 28s\n","Train loss: 2.5190156482914996 | Train perplexity: 12.416368572575989\n","Valid loss: 2.7784290313720703 | Valid perplexity: 16.09371835180147\n","Epoch: 7 | Time: 0m 28s\n","Train loss: 2.4820566376925566 | Train perplexity: 11.96584854400247\n","Valid loss: 2.8260845243930817 | Valid perplexity: 16.879241011666565\n","Epoch: 8 | Time: 0m 28s\n","Train loss: 2.393401177969273 | Train perplexity: 10.950675873546876\n","Valid loss: 3.0057299733161926 | Valid perplexity: 20.20095687462535\n","Epoch: 9 | Time: 0m 28s\n","Train loss: 2.3277443468833283 | Train perplexity: 10.25478418947722\n","Valid loss: 2.7504774928092957 | Valid perplexity: 15.650102911970404\n","Epoch: 10 | Time: 0m 28s\n","Train loss: 2.2664638148530463 | Train perplexity: 9.645233107954612\n","Valid loss: 2.7965654134750366 | Valid perplexity: 16.388263091629074\n","Epoch: 11 | Time: 0m 28s\n","Train loss: 2.194908532802229 | Train perplexity: 8.979179718850036\n","Valid loss: 2.8912895023822784 | Valid perplexity: 18.01652698336398\n","Epoch: 12 | Time: 0m 28s\n","Train loss: 2.1471146035299427 | Train perplexity: 8.560123379390214\n","Valid loss: 2.825638473033905 | Valid perplexity: 16.87171368218402\n","Epoch: 13 | Time: 0m 28s\n","Train loss: 2.0783692598342896 | Train perplexity: 7.991426342745632\n","Valid loss: 2.900662451982498 | Valid perplexity: 18.186188856748846\n","Epoch: 14 | Time: 0m 28s\n","Train loss: 2.029898477545919 | Train perplexity: 7.613313397284385\n","Valid loss: 2.650618463754654 | Valid perplexity: 14.162795112759676\n","Epoch: 15 | Time: 0m 28s\n","Train loss: 1.9684726330677318 | Train perplexity: 7.159732593995029\n","Valid loss: 2.813238501548767 | Valid perplexity: 16.66379666028198\n","Epoch: 16 | Time: 0m 28s\n","Train loss: 1.9311225020412832 | Train perplexity: 6.897248072944181\n","Valid loss: 2.9390400648117065 | Valid perplexity: 18.897697038220624\n","Epoch: 17 | Time: 0m 28s\n","Train loss: 1.8671899519302773 | Train perplexity: 6.4700895675309775\n","Valid loss: 2.7817061841487885 | Valid perplexity: 16.146546441165725\n","Epoch: 18 | Time: 0m 28s\n","Train loss: 1.8098093596849147 | Train perplexity: 6.109282645637987\n","Valid loss: 2.760519415140152 | Valid perplexity: 15.808051757605119\n","Epoch: 19 | Time: 0m 28s\n","Train loss: 1.7737439151377405 | Train perplexity: 5.892874535134238\n","Valid loss: 2.86311674118042 | Valid perplexity: 17.516034896132812\n","Epoch: 20 | Time: 0m 28s\n","Train loss: 1.7149440900869832 | Train perplexity: 5.556364847801556\n","Valid loss: 2.9820898473262787 | Valid perplexity: 19.72900419859693\n"]}]},{"cell_type":"code","source":["model.load_state_dict(torch.load('/content/seq2seq_ver1.pt'))\n","\n","test_loss = evaluate(model, test_iterator, criterion)\n","\n","print(f'Test loss: {test_loss:.3f} | Test Perplexity: {math.exp(test_loss):.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PeBBMujbnMct","executionInfo":{"status":"ok","timestamp":1658690280939,"user_tz":-540,"elapsed":698,"user":{"displayName":"김별희","userId":"06602448826503759202"}},"outputId":"694f52a0-54e6-4e0c-c332-c87d78e9353f"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["Test loss: 2.840 | Test Perplexity: 17.110\n"]}]},{"cell_type":"markdown","source":["## 5. Inference"],"metadata":{"id":"FVC9VB2tjs7l"}},{"cell_type":"code","source":["model.load_state_dict(torch.load('/content/seq2seq_ver1.pt'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lTIuwm9Ymwfz","executionInfo":{"status":"ok","timestamp":1658690280939,"user_tz":-540,"elapsed":5,"user":{"displayName":"김별희","userId":"06602448826503759202"}},"outputId":"56f1cd34-0250-4ce4-852a-b3fa5346243e"},"execution_count":68,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":68}]},{"cell_type":"code","source":["def translate(sent, src_field, trg_field, model, device, max_len=45):\n","  model.eval()\n","\n","  if isinstance(sent, str):\n","    spacy_de = spacy.load(\"nl_core_news_sm\")\n","    tokens = [token.text.lower() for token in spacy_de(sent)]\n","  else:\n","    tokens = [token.lower() for token in sent]\n","  \n","  # <SOS> <EOS> token\n","  tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n","  print(f\"soruce token: {tokens}\")\n","\n","  src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n","  print(f\"source token idx: {src_indexes}\")\n","\n","  src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n","\n","  with torch.no_grad():\n","    hidden, cell = model.encoder(src_tensor)\n","  \n","  trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]] # 처음에는 <SOS>토큰을 갖고 있음\n","\n","  for i in range(max_len):\n","    # 이전 출력 단어가 현재 단어로 입력되도록\n","    trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device) \n","\n","    with torch.no_grad():\n","      output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n","\n","    pred_token = output.argmax(1).item()\n","    trg_indexes.append(pred_token)\n","\n","    if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n","      break\n","    \n","  trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n","\n","  return trg_tokens"],"metadata":{"id":"hP7Oyl79jB9f","executionInfo":{"status":"ok","timestamp":1658690280939,"user_tz":-540,"elapsed":3,"user":{"displayName":"김별희","userId":"06602448826503759202"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["example_idx = 10\n","\n","src = vars(test_dataset.examples[example_idx])['src']\n","trg = vars(test_dataset.examples[example_idx])['trg']\n","\n","print(f\"Source Dutch: {src}\")\n","print(\" \".join(translate(src, SRC, TRG, model, device)))\n","print(trg)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bUJgXoh3l8z_","executionInfo":{"status":"ok","timestamp":1658690280939,"user_tz":-540,"elapsed":3,"user":{"displayName":"김별희","userId":"06602448826503759202"}},"outputId":"dc91b038-107f-48dd-ec0f-6461010f1251"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["Source Dutch: ['.', 'freien', 'im', 'tag', 'schönen', 'einen', 'genießen', 'sohn', 'kleiner', 'ihr', 'und', 'mutter', 'eine']\n","soruce token: ['<SOS>', '.', 'freien', 'im', 'tag', 'schönen', 'einen', 'genießen', 'sohn', 'kleiner', 'ihr', 'und', 'mutter', 'eine', '<EOS>']\n","source token idx: [2, 4, 87, 20, 200, 781, 19, 566, 625, 70, 134, 10, 365, 8, 3]\n","<SOS> a mother and her girl are enjoying themselves on a outdoor day . <EOS>\n","['a', 'mother', 'and', 'her', 'young', 'song', 'enjoying', 'a', 'beautiful', 'day', 'outside', '.']\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"r5ZsKt3XnUBH"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Seq2Seq Implementation.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1g9LVAGunzdrIY6L5zolG1yytuw-XBeiE","authorship_tag":"ABX9TyNVp+DTBmiAMXjhY6bwUk4Q"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}